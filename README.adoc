= MCP with OpenShift - GitOps Repository
:toc: macro
:toclevels: 3
:icons: font

Per-user Helm charts for the **MCP with OpenShift** lab, deployed via ArgoCD ApplicationSets during user provisioning.

toc::[]

== Deployment Architecture

This lab uses the **Infra / Tenant** pattern for OCP Sandbox deployments. Shared cluster infrastructure is provisioned once (infra), and per-user resources are deployed on demand (tenant).

[source]
----
                         ┌─────────────────────────────────────────────┐
                         │          OCP Cluster (Pre-provisioned)      │
                         │                                             │
  ┌───────────────────┐  │  ┌──────────────────────────────────────┐   │
  │ Cluster           │  │  │  Infra (Cluster Provisioner)         │   │
  │ Provisioner       │──┼─▶│                                      │   │
  │ (run once per     │  │  │  - Keycloak (RHBK + OAuth)           │   │
  │  cluster, CI or   │  │  │                                      │   │
  │  manual)          │  │  │                                      │   │
  └───────────────────┘  │  │  - Gitea Operator                    │   │
                         │  │  - OpenShift Pipelines (Tekton)       │   │
                         │  │  - OpenShift GitOps (ArgoCD)          │   │
                         │  │  - ToolHive Operator                  │   │
                         │  │  - User Workload Monitoring            │   │
                         │  │  - CloudNativePG Operator              │   │
                         │  │  - OCP Console Embed (iframe CSP)      │   │
                         │  └──────────────────────────────────────┘   │
                         │                                             │
  ┌───────────────────┐  │  ┌──────────────────────────────────────┐   │
  │ User Provisioner  │  │  │  Tenant (Per-User Resources)         │   │
  │ (on each order    │──┼─▶│                                      │   │
  │  via Sandbox API) │  │  │  - ArgoCD AppProject                 │   │
  └───────────────────┘  │  │  - LiteLLM Virtual Key               │   │
                         │  │  - Per-user Gitea instance            │   │
                         │  │  - MCP OpenShift Server (ToolHive)    │   │
                         │  │  - MCP Gitea Server (ToolHive)        │   │
                         │  │  - LibreChat (AI UI)                  │   │
                         │  │  - Pipeline Failure Agent             │   │
                         │  │  - Showroom (Lab Guide)               │   │
                         │  └──────────────────────────────────────┘   │
                         └─────────────────────────────────────────────┘
----

=== Infra (Cluster Provisioner)

Shared infrastructure deployed once per cluster (manually or via CI). Managed via the
https://github.com/rhpds/rhpds.ocpsandbox_mcp_with_openshift[rhpds.ocpsandbox_mcp_with_openshift]
collection's `tests/e2e/cluster-provision.yml` playbook.

Key decisions:

* **Keycloak (RHBK)** as OCP OAuth provider -- OCPSandbox API creates users dynamically when `keycloak: "yes"` is set on the cluster pool.
* **Gitea Operator** installed cluster-wide -- each user gets their own Gitea instance via a `Gitea` CR.
* **ToolHive Operator** installed cluster-wide -- MCP servers are deployed as `MCPServer` CRs per user.
* **OCP Console Embed** patches IngressController CSP headers so the OpenShift console can be embedded in a Showroom iframe.

=== Cluster Authentication

The AgnosticD `config: namespace` framework sets `K8S_AUTH_HOST`, `K8S_AUTH_API_KEY`, and `K8S_AUTH_VERIFY_SSL` environment variables on every workload role via `apply: environment:`. This means workload roles using `kubernetes.core` modules work without a kubeconfig file -- the env vars provide cluster access directly.

As a fallback (for e2e tests or environments outside the AgnosticD framework), the MCP user role also writes a kubeconfig to disk at the path specified by `KUBECONFIG` env var or `~/.kube/config`.

=== Tenant (User Provisioner)

Per-user resources deployed on each lab order via the OCPSandbox API using `config: namespace`.

**This repository contains the Helm charts** for the tenant-level components. ArgoCD ApplicationSets
(created by the `ocp4_workload_ocpsandbox_mcp_user` role) deploy these charts into per-user namespaces.

Each user gets isolated namespaces:

[cols="2,3"]
|===
|Namespace |Component

|`mcp-openshift-{user}`
|OpenShift MCP Server (via ToolHive MCPServer CR)

|`mcp-gitea-{user}`
|Gitea MCP Server (via ToolHive MCPServer CR)

|`librechat-{user}`
|LibreChat AI UI + Meilisearch + MongoDB

|`agent-{user}`
|Pipeline Failure Agent

|`gitea-{user}`
|Per-user Gitea instance (operator CR, not a Helm chart)

|`showroom-{user}`
|Showroom Lab Guide
|===

== How It Works

[source]
----
User orders lab
       │
       ▼
OCPSandbox API
       │
       ├── Creates Keycloak user (keycloak: "yes")
       ├── Creates sandbox namespace with quota
       │
       ▼
AgnosticD Workloads (config: namespace)
       │
       ├── 1. ArgoCD AppProject (per-user RBAC)
       ├── 2. LiteLLM Virtual Key (LLM API access)
       ├── 3. MCP User Role:
       │      ├── Discover cluster ingress domain
       │      ├── Write kubeconfig for downstream roles
       │      ├── Deploy per-user Gitea (operator CR)
       │      ├── Create Gitea user + migrate repos
       │      ├── Create ArgoCD ApplicationSets ──────┐
       │      └── Save user_info for Showroom          │
       ├── 4. OCP Console Embed (idempotent)           │
       └── 5. Showroom (lab guide UI)                  │
                                                       │
                                    ┌──────────────────┘
                                    ▼
                              ArgoCD syncs
                              Helm charts from
                              THIS REPO (tenant/)
                                    │
                    ┌───────────────┼───────────────┐
                    │               │               │
                    ▼               ▼               ▼
             mcp-openshift    mcp-gitea      librechat + agent
             (ToolHive)       (ToolHive)     (Helm charts)
----

== Agent Architecture

The Pipeline Failure Agent connects MCP servers to an LLM for automated pipeline failure analysis:

[source]
----
┌─────────────────┐     POST /report-failure     ┌─────────────────┐
│  Tekton          │ ────────────────────────────▶│  Pipeline       │
│  Pipeline       │                              │  Failure Agent  │
└─────────────────┘                              └────────┬────────┘
                                                          │
                                     ┌────────────────────┼────────────────────┐
                                     │                    │                    │
                                     ▼                    ▼                    ▼
                            ┌────────────────┐   ┌────────────────┐   ┌────────────────┐
                            │  LiteLLM       │   │  MCP OpenShift │   │  MCP Gitea     │
                            │  (LLM API)     │   │  Server        │   │  Server        │
                            └────────────────┘   └────────────────┘   └────────────────┘
                                                          │                    │
                                                          ▼                    ▼
                                                   Pod Logs            Issue Creation
----

The agent operates in an iterative loop:

1. **Receive Failure Report** -- Pipeline sends pod details to `/report-failure`
2. **Build Prompt** -- Agent creates a prompt with pod context and examples
3. **LLM Analysis** -- Model analyzes the situation and requests tools
4. **Tool Execution** -- Agent executes requested tools via MCP servers (`pods_log`, `create_issue`)
5. **Iterate** -- Process continues until the model completes or max iterations reached
6. **Return Result** -- Agent returns the final result (usually issue URL)

== Repository Structure

[source]
----
ocpsandbox-mcp-with-openshift-gitops/
│
├── tenant/                        # Per-user Helm charts (deployed by ArgoCD)
│   ├── agent/                     # Pipeline Failure Agent
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   ├── librechat/                 # LibreChat config overrides
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   ├── mcp-gitea/                 # Gitea MCP Server (ToolHive MCPServer CR)
│   │   ├── Chart.yaml
│   │   ├── values.yaml
│   │   └── templates/
│   └── mcp-openshift/             # OpenShift MCP Server (ToolHive MCPServer CR)
│       ├── Chart.yaml
│       ├── values.yaml
│       └── templates/
│
├── agent/                         # Agent source code
│   ├── main.py                    # FastAPI server + agent logic
│   ├── mcp_client.py              # MCP server client (SSE + streamable-http)
│   ├── requirements.txt
│   ├── Containerfile
│   └── tests/
│
└── README.adoc                    # This file
----

The `tenant/` directory follows the https://github.com/rhpds/ci-template-gitops[rhpds/ci-template-gitops] pattern where:

* `infra/` = cluster-scoped resources (not used here -- infra is handled by the cluster provisioner playbook)
* `tenant/` = per-user (namespace-scoped) resources

== Related Repositories

[cols="2,3"]
|===
|Repository |Purpose

|https://github.com/rhpds/rhpds.ocpsandbox_mcp_with_openshift[rhpds.ocpsandbox_mcp_with_openshift]
|Ansible collection with `ocp4_workload_ocpsandbox_mcp_user` role + e2e tests + cluster provisioner

|https://github.com/agnosticd/namespaced_workloads[agnosticd/namespaced_workloads]
|Generic sandbox roles (`argocd_user`, `keycloak_user`, `gitea_user`)

|https://github.com/rhpds/rhpds.litellm_virtual_keys[rhpds.litellm_virtual_keys]
|LiteLLM virtual key provisioning

|https://github.com/rhpds/lb1726-mcp-showroom[rhpds/lb1726-mcp-showroom]
|Showroom lab guide content

|https://github.com/rhpds/ocpsandbox-mcp-with-openshift-gitops[rhpds/ocpsandbox-mcp-with-openshift-gitops]
|This repo -- GitOps charts
|===

== MCP Servers

[cols="1,1,1,2"]
|===
|Server |Transport |Image |Purpose

|OpenShift
|SSE
|`quay.io/containers/kubernetes_mcp_server:latest`
|K8s/OCP resource management, pod logs, exec

|Gitea
|streamable-http
|`docker.gitea.com/gitea-mcp-server:latest`
|Issue management, repository operations, PR workflows
|===

== Configuration

=== Agent Environment Variables

[cols="2,3,1"]
|===
|Variable |Description |Default

|`LITELLM_URL`
|Base URL for the LiteLLM API endpoint
|_(required)_

|`LITELLM_API_KEY`
|API key for LiteLLM authentication
|_(required)_

|`LITELLM_MODEL`
|Model identifier to use for analysis
|`openai/Llama-4-Scout-17B-16E-W4A16`

|`MCP_OPENSHIFT_URL`
|URL for the OpenShift MCP server
|_(required)_

|`MCP_OPENSHIFT_TRANSPORT`
|Transport type for OpenShift MCP server
|`sse`

|`MCP_GITEA_URL`
|URL for the Gitea MCP server
|_(required)_

|`MCP_GITEA_TRANSPORT`
|Transport type for Gitea MCP server
|`streamable-http`

|`GITEA_OWNER`
|Gitea repository owner for issue creation
|`user1`

|`GITEA_REPO`
|Gitea repository name for issue creation
|`mcp`

|`PORT`
|Server listening port
|`8000`
|===

== Local Development

=== Agent

[source,bash]
----
cd agent
pip install -r requirements.txt

export LITELLM_URL="http://your-litellm-endpoint"
export LITELLM_API_KEY="your-api-key"
export MCP_OPENSHIFT_URL="http://mcp-openshift-server/sse"
export MCP_GITEA_URL="http://mcp-gitea-server/mcp"

python main.py
----

=== Container Build

[source,bash]
----
cd agent
podman build -t pipeline-agent -f Containerfile .
----

=== Testing

[source,bash]
----
cd agent
pip install -r requirements.txt
pytest tests/ -v --cov=. --cov-report=term-missing
----
